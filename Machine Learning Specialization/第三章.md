## 3.1线性回归模型

比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。
![alt text](image-9.png)

它被称作监督学习是因为对于每个数据来说，我们给出了“正确的答案”，即告诉我们：根据我们的数据来说，房子实际的价格是多少，而且，更具体来说，这是一个**回归问题**。回归一词指的是，我们根据之前的数据预测出一个准确的输出值，对于这个例子就是价格。

假使我们回归问题的训练集（Training Set）如下表所示：
![alt text](image-10.png)
我们将要用来描述这个回归问题的标记如下:

$m$代表训练集中实例的数量

$x$代表特征/输入变量

$y$代表目标变量/输出变量

$(x,y)$代表训练集中的实例

$(x^i,y^i)$代表第i个观察实例

![alt text](image-12.png)

$f(x)=wx+b$

因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。
## 3.2代价函数

![alt text](image-13.png)
在线性回归中我们有一个像这样的训练集,我们的进行预测的函数,是这样的线性函数:$f$($x$)=$w$$x$+$b$。接下来我们会引入一些术语我们现在要做的便是为我们的模型选择合适的参数（parameters）$w$和$b$,在房价问题这个例子中便是直线的斜率和在$y$ 轴上的截距。

我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距误差（modeling error）。
![alt text](image-14.png)

我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数

$J(w,b)={1\over 2m}\sum_1^m(f_{w,b}(x)^i-y^i)^2$
最小。

代价函数衡量模型预测与y实际真实值之间的差异。

当我们令$b$为0,$f_w(x)$和$J(w)$的关系如图:
![alt text](image-16.png)
参数$w$的每个值对应左图上不同的直线拟合$f(x)$。对于给定的训练集，对$w$值的选择对应右图的一个点。
![alt text](image-17.png)

$J$是衡量平方误差有多大的代价函数,所以选择一个能够最小化这些平方误差的$w$,使他们尽可能的小,将为我们提供一个更好的模型。

当有两个变量$w$和$b$时,图形会变得复杂
![alt text](image-18.png)

![alt text](image-19.png)

![alt text](image-20.png)
